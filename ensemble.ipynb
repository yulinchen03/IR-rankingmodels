{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-29T18:39:24.151355100Z",
     "start_time": "2025-03-29T18:39:17.120607Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chena\\PycharmProjects\\IR-rankingmodels\\.venv\\Lib\\site-packages\\beir\\datasets\\data_loader.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from IRutils.inference import evaluate, evaluate_average_ensemble, evaluate_conditional_ensemble, evaluate_weighted_average_ensemble\n",
    "from IRutils.load_data import load, preprocess\n",
    "from ir_measures import calc_aggregate\n",
    "from tqdm.notebook import tqdm\n",
    "from IRutils.models import load_model, load_models\n",
    "from ir_measures import nDCG, AP, P, R, RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\long_queries.pth...\n",
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\medium_queries.pth...\n",
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\short_queries.pth...\n",
      "Ensembling models from models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "dataset_name = 'fiqa'\n",
    "length_setting = 'full'\n",
    "metrics = [nDCG@10, nDCG@100, AP@10, AP@100, P@10, R@10, P@100, R@100, RR]\n",
    "max_len_doc = 512\n",
    "random_state = 42\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = f'models\\\\{model_name}\\\\{dataset_name}'\n",
    "\n",
    "\n",
    "models = load_models(model_dir, model_name, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T18:39:26.348716500Z",
     "start_time": "2025-03-29T18:39:24.154355500Z"
    }
   },
   "id": "af699fb18d36e1d4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/57638 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f79bb4acd9e747908c69fbe6d55621ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/57638 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5db1c73bcdf74b378f4858cdabcfe345"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test set available!\n",
      "Loading complete!\n"
     ]
    }
   ],
   "source": [
    "train_available, docs, queries, qrels, docs_test, queries_test, qrels_test  = load(dataset_name)\n",
    "print('Loading complete!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T18:39:27.183312900Z",
     "start_time": "2025-03-29T18:39:26.344211400Z"
    }
   },
   "id": "fa2e670e37ecdcb0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import sys\n",
    "from IRutils.load_data import calculate_percentiles\n",
    "\n",
    "query_lengths = [len(txt.split()) for txt in list(queries.values())]\n",
    "t1, t2 = calculate_percentiles(query_lengths)\n",
    "ranges = {'short': (1, t1), 'medium': (t1, t2), 'long': (t2, sys.maxsize), 'full': (1, sys.maxsize)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T18:39:27.184312900Z",
     "start_time": "2025-03-29T18:39:27.172253400Z"
    }
   },
   "id": "497c8e3d25ba66c2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5500\n",
      "test size: 648\n",
      "Example query from full subset:\n",
      "('11104', 'Selling a stock for gain to offset other stock loss')\n",
      "Length of subset of full validation queries: 1100\n",
      "Length of subset of full training queries: 4399\n",
      "Length of subset of full queries: 5499\n",
      "Number of negatives in qrels: 0\n",
      "Creating training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4399/4399 [00:13<00:00, 335.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:03<00:00, 330.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating testing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 648/648 [00:01<00:00, 353.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if train_available:\n",
    "    train_loader, val_loader, test_loader, split_queries_test, split_qrels_test = preprocess(queries, docs, qrels, model_name, length_setting, train_available, \n",
    "                                                       queries_test=queries_test, docs_test=docs_test, qrels_test=qrels_test, \n",
    "                                                       max_len_doc=max_len_doc, random_state=random_state)\n",
    "else:\n",
    "    train_loader, val_loader, test_loader, split_queries_test, split_qrels_test = preprocess(queries, docs, qrels, model_name, length_setting, train_available, \n",
    "                                                       max_len_doc=max_len_doc, random_state=random_state)\n",
    "    \n",
    "print('Preprocessing complete!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T18:39:45.963415Z",
     "start_time": "2025-03-29T18:39:27.178312500Z"
    }
   },
   "id": "835d7b2859eb13c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate on baseline model (trained on all query lengths)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "844dae48da2b7a9e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# model_dir = os.path.join(os.getcwd(), f'models/{model_name}/{dataset_name}')\n",
    "# model_path = os.path.join(model_dir, f'{length_setting}_queries.pth')\n",
    "# model = load_model(model_path, model_name, device)\n",
    "# \n",
    "# if train_available:\n",
    "#     metric_scores = evaluate(model, test_loader, device, qrels_test)\n",
    "# else:\n",
    "#     metric_scores = evaluate(model, test_loader, device, split_qrels_test)\n",
    "#     \n",
    "# for metric in metrics:\n",
    "#     print(f'Metric {metric} score: {metric_scores[metric]:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T18:39:45.964741100Z",
     "start_time": "2025-03-29T18:39:45.961399600Z"
    }
   },
   "id": "b23ee5c92ee4ae94"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# save_dir = f\"results/{model_name}/{dataset_name}\"\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "# \n",
    "# # Save results to a file\n",
    "# with open(f\"{os.path.join(save_dir, 'full_queries.txt')}\", \"w\") as f:\n",
    "#     f.write(f\"Evaluation of results for {model_name} model trained on {dataset_name} dataset:\\n\")\n",
    "#     f.write(f\"normalized Discounted Cumulative Gain@10: {metric_scores[nDCG@10]:.4f}\\n\")\n",
    "#     f.write(f\"normalized Discounted Cumulative Gain@100: {metric_scores[nDCG@100]:.4f}\\n\")\n",
    "#     f.write(f\"\\n\")\n",
    "#     f.write(f\"[Mean] Average Precision@10: {metric_scores[AP@10]:.4f}\\n\")\n",
    "#     f.write(f\"[Mean] Average Precision@100: {metric_scores[AP@100]:.4f}\\n\")\n",
    "#     f.write(f\"\\n\")\n",
    "#     f.write(f\"Precision@10: {metric_scores[P@10]:.4f}\\n\")\n",
    "#     f.write(f\"Recall@10: {metric_scores[R@10]:.4f}\\n\")\n",
    "#     f.write(f\"\\n\")\n",
    "#     f.write(f\"Precision@100: {metric_scores[P@100]:.4f}\\n\")\n",
    "#     f.write(f\"Recall@100: {metric_scores[R@100]:.4f}\\n\")\n",
    "#     f.write(f\"\\n\")\n",
    "#     f.write(f\"[Mean] Reciprocal Rank: {metric_scores[RR]:.4f}\\n\")\n",
    "#     f.write(f\"\\n\")\n",
    "#     f.write(f\"----------------------------------------------------\\n\")\n",
    "#     f.write(f\"\\n\")\n",
    "#     f.write(f\"Explanation of metrics:\\n\")\n",
    "#     f.write(f\"NDCG@k (Normalized Discounted Cumulative Gain: Ranking Quality | Prioritizes highly relevant documents appearing earlier in the ranking.\\n\")\n",
    "#     f.write(f\"MAP (Mean Average Precision): Overall Relevance | Measures ranking precision across all relevant documents. Best for small-scale retrieval tasks.\\n\")\n",
    "#     f.write(f\"Precision@k: Relevance | Measures how many of the top-k documents are relevant. Works well in precision-sensitive applications.\\n\")\n",
    "#     f.write(f\"Recall@k: Coverage | Measures how many relevant documents appear in the top-k results. Important in recall-sensitive tasks.\\n\")\n",
    "#     f.write(f\"MRR (Mean Reciprocal Rank): Single Relevant Result | Focuses on ranking the first relevant document. Good for QA tasks.\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T18:39:46.001128200Z",
     "start_time": "2025-03-29T18:39:45.965749100Z"
    }
   },
   "id": "57e6feead647abcc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate on ensemble of models (Averaging method)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9a5305fad7d585d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\long_queries.pth...\n",
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\medium_queries.pth...\n",
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\short_queries.pth...\n",
      "Ensembling models from models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa!\n",
      "{'long': TripletRankerModel(\n",
      "  (model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 312)\n",
      "      (token_type_embeddings): Embedding(2, 312)\n",
      "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-3): 4 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
      "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=312, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "), 'medium': TripletRankerModel(\n",
      "  (model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 312)\n",
      "      (token_type_embeddings): Embedding(2, 312)\n",
      "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-3): 4 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
      "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=312, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "), 'short': TripletRankerModel(\n",
      "  (model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 312, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 312)\n",
      "      (token_type_embeddings): Embedding(2, 312)\n",
      "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-3): 4 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=312, out_features=1200, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1200, out_features=312, bias=True)\n",
      "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=312, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4265/4265 [20:38<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric nDCG@10 score: 0.6886\n",
      "Metric nDCG@100 score: 0.7378\n",
      "Metric AP@10 score: 0.5825\n",
      "Metric AP@100 score: 0.6080\n",
      "Metric P@10 score: 0.1985\n",
      "Metric R@10 score: 0.8554\n",
      "Metric P@100 score: 0.0260\n",
      "Metric R@100 score: 0.9961\n",
      "Metric RR score: 0.6841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# models = load_models(model_dir, model_name, device)\n",
    "# \n",
    "# if train_available:\n",
    "#     metric_scores = evaluate_average_ensemble(models, test_loader, device, qrels_test)\n",
    "# else:\n",
    "#     metric_scores = evaluate_average_ensemble(models, test_loader, device, split_qrels_test)\n",
    "#     \n",
    "# for metric in metrics:\n",
    "#     print(f'Metric {metric} score: {metric_scores[metric]:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T19:00:26.222342100Z",
     "start_time": "2025-03-29T18:39:45.982127900Z"
    }
   },
   "id": "3978978feaf4178f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# save_dir = f\"results/{model_name}/{dataset_name}\"\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "# \n",
    "# # Save results to a file\n",
    "# with open(f\"{os.path.join(save_dir, 'ensemble-avg.txt')}\", \"w\") as f:\n",
    "#     f.write(f\"Evaluation of results for {model_name} model trained on {dataset_name} dataset:\\n\")\n",
    "#     f.write(f\"normalized Discounted Cumulative Gain@10: {metric_scores[nDCG@10]:.4f}\\n\")\n",
    "#     f.write(f\"normalized Discounted Cumulative Gain@100: {metric_scores[nDCG@100]:.4f}\\n\")\n",
    "#     f.write(f\"\\n\")\n",
    "#     f.write(f\"[Mean] Average Precision@10: {metric_scores[AP@10]:.4f}\\n\")\n",
    "#     f.write(f\"[Mean] Average Precision@100: {metric_scores[AP@100]:.4f}\\n\")\n",
    "#     f.write(f\"\\n\")\n",
    "#     f.write(f\"Precision@10: {metric_scores[P@10]:.4f}\\n\")\n",
    "#     f.write(f\"Recall@10: {metric_scores[R@10]:.4f}\\n\")\n",
    "#     f.write(f\"\\n\")\n",
    "#     f.write(f\"Precision@100: {metric_scores[P@100]:.4f}\\n\")\n",
    "#     f.write(f\"Recall@100: {metric_scores[R@100]:.4f}\\n\")\n",
    "#     f.write(f\"\\n\")\n",
    "#     f.write(f\"[Mean] Reciprocal Rank: {metric_scores[RR]:.4f}\\n\")\n",
    "#     f.write(f\"\\n\")\n",
    "#     f.write(f\"----------------------------------------------------\\n\")\n",
    "#     f.write(f\"\\n\")\n",
    "#     f.write(f\"Explanation of metrics:\\n\")\n",
    "#     f.write(f\"NDCG@k (Normalized Discounted Cumulative Gain: Ranking Quality | Prioritizes highly relevant documents appearing earlier in the ranking.\\n\")\n",
    "#     f.write(f\"MAP (Mean Average Precision): Overall Relevance | Measures ranking precision across all relevant documents. Best for small-scale retrieval tasks.\\n\")\n",
    "#     f.write(f\"Precision@k: Relevance | Measures how many of the top-k documents are relevant. Works well in precision-sensitive applications.\\n\")\n",
    "#     f.write(f\"Recall@k: Coverage | Measures how many relevant documents appear in the top-k results. Important in recall-sensitive tasks.\\n\")\n",
    "#     f.write(f\"MRR (Mean Reciprocal Rank): Single Relevant Result | Focuses on ranking the first relevant document. Good for QA tasks.\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T19:03:08.418805900Z",
     "start_time": "2025-03-29T19:03:08.402482700Z"
    }
   },
   "id": "9fd52cf03cb83f67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate on ensemble of models (Selective method)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e6bb7cedb44a164"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\long_queries.pth...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m model_dir = \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mmodels\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mdataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m models = \u001B[43mload_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m train_available:\n\u001B[32m      6\u001B[39m     metric_scores = evaluate_conditional_ensemble(models, t1, t2, test_loader, device, qrels_test, queries_test)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\IR-rankingmodels\\IRutils\\models.py:40\u001B[39m, in \u001B[36mload_models\u001B[39m\u001B[34m(model_dir, model_name, device)\u001B[39m\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m model_paths:\n\u001B[32m     39\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mLoading model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m...\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m     model = \u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33mshort\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m path.split(\u001B[33m'\u001B[39m\u001B[33m/\u001B[39m\u001B[33m'\u001B[39m)[-\u001B[32m1\u001B[39m]:\n\u001B[32m     42\u001B[39m         models[\u001B[33m'\u001B[39m\u001B[33mshort\u001B[39m\u001B[33m'\u001B[39m] = model\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\IR-rankingmodels\\IRutils\\models.py:27\u001B[39m, in \u001B[36mload_model\u001B[39m\u001B[34m(path, model_name, device)\u001B[39m\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload_model\u001B[39m(path, model_name, device):\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m     model = \u001B[43mTripletRankerModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m     model.load_state_dict(torch.load(path, map_location=device))\n\u001B[32m     29\u001B[39m     model.to(device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\IR-rankingmodels\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1343\u001B[39m, in \u001B[36mModule.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1340\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1341\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1343\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\IR-rankingmodels\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    901\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    902\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m903\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    905\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    906\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    907\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    908\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    913\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    914\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\IR-rankingmodels\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    901\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    902\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m903\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    905\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    906\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    907\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    908\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    913\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    914\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\IR-rankingmodels\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    901\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    902\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m903\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    905\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    906\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    907\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    908\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    913\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    914\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\IR-rankingmodels\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    926\u001B[39m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[32m    927\u001B[39m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[32m    928\u001B[39m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[32m    929\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m     param_applied = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    931\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001B[32m    933\u001B[39m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\IR-rankingmodels\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1329\u001B[39m, in \u001B[36mModule.to.<locals>.convert\u001B[39m\u001B[34m(t)\u001B[39m\n\u001B[32m   1322\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t.dim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m):\n\u001B[32m   1323\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m t.to(\n\u001B[32m   1324\u001B[39m             device,\n\u001B[32m   1325\u001B[39m             dtype \u001B[38;5;28;01mif\u001B[39;00m t.is_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t.is_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1326\u001B[39m             non_blocking,\n\u001B[32m   1327\u001B[39m             memory_format=convert_to_format,\n\u001B[32m   1328\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1329\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1330\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1331\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1332\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1333\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1334\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1335\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) == \u001B[33m\"\u001B[39m\u001B[33mCannot copy out of meta tensor; no data!\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mRuntimeError\u001B[39m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model_dir = f'models\\\\{model_name}\\\\{dataset_name}'\n",
    "\n",
    "models = load_models(model_dir, model_name, device)\n",
    "    \n",
    "if train_available:\n",
    "    metric_scores = evaluate_conditional_ensemble(models, t1, t2, test_loader, device, qrels_test, queries_test)\n",
    "else:\n",
    "    metric_scores = evaluate_conditional_ensemble(models, t1, t2, test_loader, device, split_qrels_test, split_queries_test)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f'Metric {metric} score: {metric_scores[metric]:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-29T19:18:12.781318200Z",
     "start_time": "2025-03-29T19:18:12.377551300Z"
    }
   },
   "id": "ea3b718cf8c07a1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_dir = f\"results/{model_name}/{dataset_name}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save results to a file\n",
    "with open(f\"{os.path.join(save_dir, 'ensemble-selective.txt')}\", \"w\") as f:\n",
    "    f.write(f\"Evaluation of results for {model_name} model trained on {dataset_name} dataset:\\n\")\n",
    "    f.write(f\"normalized Discounted Cumulative Gain@10: {metric_scores[nDCG@10]:.4f}\\n\")\n",
    "    f.write(f\"normalized Discounted Cumulative Gain@100: {metric_scores[nDCG@100]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"[Mean] Average Precision@10: {metric_scores[AP@10]:.4f}\\n\")\n",
    "    f.write(f\"[Mean] Average Precision@100: {metric_scores[AP@100]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Precision@10: {metric_scores[P@10]:.4f}\\n\")\n",
    "    f.write(f\"Recall@10: {metric_scores[R@10]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Precision@100: {metric_scores[P@100]:.4f}\\n\")\n",
    "    f.write(f\"Recall@100: {metric_scores[R@100]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"[Mean] Reciprocal Rank: {metric_scores[RR]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"----------------------------------------------------\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Explanation of metrics:\\n\")\n",
    "    f.write(f\"NDCG@k (Normalized Discounted Cumulative Gain: Ranking Quality | Prioritizes highly relevant documents appearing earlier in the ranking.\\n\")\n",
    "    f.write(f\"MAP (Mean Average Precision): Overall Relevance | Measures ranking precision across all relevant documents. Best for small-scale retrieval tasks.\\n\")\n",
    "    f.write(f\"Precision@k: Relevance | Measures how many of the top-k documents are relevant. Works well in precision-sensitive applications.\\n\")\n",
    "    f.write(f\"Recall@k: Coverage | Measures how many relevant documents appear in the top-k results. Important in recall-sensitive tasks.\\n\")\n",
    "    f.write(f\"MRR (Mean Reciprocal Rank): Single Relevant Result | Focuses on ranking the first relevant document. Good for QA tasks.\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-29T19:15:37.291262400Z"
    }
   },
   "id": "97049e4b54bda9e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate on ensemble of models (Weighted average method)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cf845baf1869443"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_dir = f'models\\\\{model_name}\\\\{dataset_name}'\n",
    "models = load_models(model_dir, model_name, device)\n",
    "\n",
    "weights_config = {'short': [0.6, 0.2, 0.2], # Weights for [short, medium, long] models when query is short\n",
    "                  'medium': [0.2, 0.6, 0.2],# Weights when query is medium\n",
    "                  'long': [0.2, 0.2, 0.6]  # Weights when query is long\n",
    "                 }\n",
    "    \n",
    "if train_available:\n",
    "    metric_scores = evaluate_weighted_average_ensemble(models, weights_config, t1, t2, test_loader, device, qrels_test, queries_test)\n",
    "else:\n",
    "    metric_scores = evaluate_weighted_average_ensemble(models, weights_config, t1, t2, test_loader, device, split_qrels_test, split_queries_test)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f'Metric {metric} score: {metric_scores[metric]:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-29T19:15:37.292263400Z"
    }
   },
   "id": "21d25cc97dc75416"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_dir = f\"results/{model_name}/{dataset_name}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save results to a file\n",
    "with open(f\"{os.path.join(save_dir, 'ensemble-weighted.txt')}\", \"w\") as f:\n",
    "    f.write(f\"Evaluation of results for {model_name} model trained on {dataset_name} dataset:\\n\")\n",
    "    f.write(f\"normalized Discounted Cumulative Gain@10: {metric_scores[nDCG@10]:.4f}\\n\")\n",
    "    f.write(f\"normalized Discounted Cumulative Gain@100: {metric_scores[nDCG@100]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"[Mean] Average Precision@10: {metric_scores[AP@10]:.4f}\\n\")\n",
    "    f.write(f\"[Mean] Average Precision@100: {metric_scores[AP@100]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Precision@10: {metric_scores[P@10]:.4f}\\n\")\n",
    "    f.write(f\"Recall@10: {metric_scores[R@10]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Precision@100: {metric_scores[P@100]:.4f}\\n\")\n",
    "    f.write(f\"Recall@100: {metric_scores[R@100]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"[Mean] Reciprocal Rank: {metric_scores[RR]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"----------------------------------------------------\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Explanation of metrics:\\n\")\n",
    "    f.write(f\"NDCG@k (Normalized Discounted Cumulative Gain: Ranking Quality | Prioritizes highly relevant documents appearing earlier in the ranking.\\n\")\n",
    "    f.write(f\"MAP (Mean Average Precision): Overall Relevance | Measures ranking precision across all relevant documents. Best for small-scale retrieval tasks.\\n\")\n",
    "    f.write(f\"Precision@k: Relevance | Measures how many of the top-k documents are relevant. Works well in precision-sensitive applications.\\n\")\n",
    "    f.write(f\"Recall@k: Coverage | Measures how many relevant documents appear in the top-k results. Important in recall-sensitive tasks.\\n\")\n",
    "    f.write(f\"MRR (Mean Reciprocal Rank): Single Relevant Result | Focuses on ranking the first relevant document. Good for QA tasks.\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-29T19:15:37.293262400Z"
    }
   },
   "id": "7d0b3fc98aea012e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
