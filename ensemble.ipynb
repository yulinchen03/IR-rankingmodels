{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-01T15:00:16.038171800Z",
     "start_time": "2025-04-01T15:00:16.005140300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "from IRutils.load_data import calculate_percentiles\n",
    "from IRutils.inference import evaluate, evaluate_average_ensemble, evaluate_conditional_ensemble, evaluate_weighted_average_ensemble, write_results\n",
    "from IRutils.load_data import load, preprocess\n",
    "from IRutils.models import load_model, load_models\n",
    "from IRutils.plotting_utils import *\n",
    "from IRutils.weight_optimizer import precompute_validation_scores, find_optimal_weights_config\n",
    "from ir_measures import nDCG, AP, P, R, RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\long_queries.pth...\n",
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\medium_queries.pth...\n",
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\short_queries.pth...\n",
      "Ensembling models from models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa!\n"
     ]
    }
   ],
   "source": [
    "results = {'baseline': {}, 'ens-avg': {}, 'ens-select': {}, 'ens-weighted': {}, 'ens-learned-weighted': {}} # Added new key\n",
    "metric_to_optimize_weights = nDCG @ 10 # Choose the metric to optimize weights for\n",
    "weight_opt_trials = 50 # Number of Optuna trials per category (adjust as needed)\n",
    "\n",
    "model_name = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "dataset_name = 'fiqa'\n",
    "length_setting = 'full'\n",
    "metrics = [nDCG @ 3, nDCG @ 5, nDCG @ 10, RR, P @ 1, P @ 3, P @ 5, R @ 1, R @ 3, R @ 5, R @ 10]\n",
    "max_len_doc = 512\n",
    "random_state = 42\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir = f'models\\\\{model_name}\\\\{dataset_name}'\n",
    "\n",
    "models = load_models(model_dir, model_name, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T15:00:17.199238200Z",
     "start_time": "2025-04-01T15:00:16.041171100Z"
    }
   },
   "id": "af699fb18d36e1d4"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/57638 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9a49730fb2748c1a463423c9fe1a531"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/57638 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a789ca24d4744c8fa1938a4be2e68926"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test set available!\n",
      "Loading complete!\n"
     ]
    }
   ],
   "source": [
    "train_available, docs, queries, qrels, docs_test, queries_test, qrels_test  = load(dataset_name)\n",
    "print('Loading complete!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T15:00:17.851077100Z",
     "start_time": "2025-04-01T15:00:17.197235700Z"
    }
   },
   "id": "fa2e670e37ecdcb0"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "query_lengths = [len(txt.split()) for txt in list(queries.values())]\n",
    "t1, t2 = calculate_percentiles(query_lengths)\n",
    "ranges = {'short': (1, t1), 'medium': (t1, t2), 'long': (t2, sys.maxsize), 'full': (1, sys.maxsize)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T15:00:17.863766700Z",
     "start_time": "2025-04-01T15:00:17.851077100Z"
    }
   },
   "id": "497c8e3d25ba66c2"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5500\n",
      "test size: 648\n",
      "Example query from full subset:\n",
      "('11104', 'Selling a stock for gain to offset other stock loss')\n",
      "Length of subset of full validation queries: 1100\n",
      "Length of subset of full training queries: 4399\n",
      "Length of subset of full queries: 5499\n",
      "Number of negatives in qrels: 0\n",
      "Creating training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4399/4399 [00:11<00:00, 375.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:03<00:00, 331.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating testing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 648/648 [00:01<00:00, 348.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "if train_available:\n",
    "    train_loader, val_loader, test_loader, split_queries_test, split_qrels_test, query_val, qrels_val = preprocess(queries, docs, qrels, model_name, length_setting, train_available, \n",
    "                                                       queries_test=queries_test, docs_test=docs_test, qrels_test=qrels_test, \n",
    "                                                       max_len_doc=max_len_doc, random_state=random_state)\n",
    "else:\n",
    "    train_loader, val_loader, test_loader, split_queries_test, split_qrels_test, query_val, qrels_val = preprocess(queries, docs, qrels, model_name, length_setting, train_available, \n",
    "                                                       max_len_doc=max_len_doc, random_state=random_state)\n",
    "    \n",
    "print('Preprocessing complete!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T15:00:35.798888900Z",
     "start_time": "2025-04-01T15:00:17.860045700Z"
    }
   },
   "id": "835d7b2859eb13c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate on baseline model (trained on all query lengths)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "844dae48da2b7a9e"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/1067 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac585e7c9f784f50bcf505856b30a857"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric nDCG@3 score: 0.5044\n",
      "Metric nDCG@5 score: 0.5743\n",
      "Metric nDCG@10 score: 0.6474\n",
      "Metric RR score: 0.6047\n",
      "Metric P@1 score: 0.4228\n",
      "Metric P@3 score: 0.3349\n",
      "Metric P@5 score: 0.2858\n",
      "Metric R@1 score: 0.2613\n",
      "Metric R@3 score: 0.5171\n",
      "Metric R@5 score: 0.6825\n",
      "Metric R@10 score: 0.8599\n"
     ]
    }
   ],
   "source": [
    "model_dir = os.path.join(os.getcwd(), f'models/{model_name}/{dataset_name}')\n",
    "model_path = os.path.join(model_dir, f'{length_setting}_queries.pth')\n",
    "model = load_model(model_path, model_name, device)\n",
    "\n",
    "if train_available:\n",
    "    metric_scores = evaluate(model, test_loader, device, qrels_test)\n",
    "else:\n",
    "    metric_scores = evaluate(model, test_loader, device, split_qrels_test)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f'Metric {metric} score: {metric_scores[metric]:.4f}')\n",
    "\n",
    "results['baseline'] = metric_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T15:01:59.671638200Z",
     "start_time": "2025-04-01T15:00:35.801396Z"
    }
   },
   "id": "b23ee5c92ee4ae94"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully written results to results/huawei-noah/TinyBERT_General_4L_312D/fiqa\\full_queries.txt.\n"
     ]
    }
   ],
   "source": [
    "save_dir = f\"results/{model_name}/{dataset_name}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'full_queries.txt')\n",
    "\n",
    "write_results(metric_scores, save_path, model_name, dataset_name, length_setting)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T15:01:59.677830900Z",
     "start_time": "2025-04-01T15:01:59.673142300Z"
    }
   },
   "id": "57e6feead647abcc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate on ensemble of models (Averaging method)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9a5305fad7d585d"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model C:\\Users\\chena\\PycharmProjects\\IR-rankingmodels\\models/huawei-noah/TinyBERT_General_4L_312D/fiqa\\long_queries.pth...\n",
      "Loading model C:\\Users\\chena\\PycharmProjects\\IR-rankingmodels\\models/huawei-noah/TinyBERT_General_4L_312D/fiqa\\medium_queries.pth...\n",
      "Loading model C:\\Users\\chena\\PycharmProjects\\IR-rankingmodels\\models/huawei-noah/TinyBERT_General_4L_312D/fiqa\\short_queries.pth...\n",
      "Ensembling models from C:\\Users\\chena\\PycharmProjects\\IR-rankingmodels\\models/huawei-noah/TinyBERT_General_4L_312D/fiqa!\n"
     ]
    },
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/1067 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2972b1d2866c45ec965d37f90ab96681"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric nDCG@3 score: 0.6120\n",
      "Metric nDCG@5 score: 0.6602\n",
      "Metric nDCG@10 score: 0.7138\n",
      "Metric RR score: 0.7148\n",
      "Metric P@1 score: 0.5617\n",
      "Metric P@3 score: 0.4048\n",
      "Metric P@5 score: 0.3151\n",
      "Metric R@1 score: 0.3275\n",
      "Metric R@3 score: 0.6086\n",
      "Metric R@5 score: 0.7446\n",
      "Metric R@10 score: 0.8811\n"
     ]
    }
   ],
   "source": [
    "models = load_models(model_dir, model_name, device)\n",
    "\n",
    "if train_available:\n",
    "    metric_scores = evaluate_average_ensemble(models, test_loader, device, qrels_test)\n",
    "else:\n",
    "    metric_scores = evaluate_average_ensemble(models, test_loader, device, split_qrels_test)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f'Metric {metric} score: {metric_scores[metric]:.4f}')\n",
    "\n",
    "results['ens-avg'] = metric_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T15:05:56.945407800Z",
     "start_time": "2025-04-01T15:01:59.678832200Z"
    }
   },
   "id": "3978978feaf4178f"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully written results to results/huawei-noah/TinyBERT_General_4L_312D/fiqa\\ensemble-avg.txt.\n"
     ]
    }
   ],
   "source": [
    "save_dir = f\"results/{model_name}/{dataset_name}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'ensemble-avg.txt')\n",
    "\n",
    "write_results(metric_scores, save_path, model_name, dataset_name, length_setting)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T15:05:56.948609800Z",
     "start_time": "2025-04-01T15:05:56.934895200Z"
    }
   },
   "id": "9fd52cf03cb83f67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate on ensemble of models (Selective method)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e6bb7cedb44a164"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\long_queries.pth...\n",
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\medium_queries.pth...\n",
      "Loading model models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa\\short_queries.pth...\n",
      "Ensembling models from models\\huawei-noah/TinyBERT_General_4L_312D\\fiqa!\n"
     ]
    },
    {
     "data": {
      "text/plain": "Evaluating Conditional Ensemble:   0%|          | 0/1067 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2029f88ae2994b02b3429cb3b19bab81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating aggregate metrics...\n",
      "Metrics calculation complete.\n",
      "Metric nDCG@3 score: 0.4469\n",
      "Metric nDCG@5 score: 0.5044\n",
      "Metric nDCG@10 score: 0.5763\n",
      "Metric RR score: 0.5500\n",
      "Metric P@1 score: 0.3580\n",
      "Metric P@3 score: 0.2963\n",
      "Metric P@5 score: 0.2432\n",
      "Metric R@1 score: 0.2169\n",
      "Metric R@3 score: 0.4762\n",
      "Metric R@5 score: 0.6170\n",
      "Metric R@10 score: 0.7958\n"
     ]
    }
   ],
   "source": [
    "model_dir = f'models\\\\{model_name}\\\\{dataset_name}'\n",
    "\n",
    "models = load_models(model_dir, model_name, device)\n",
    "\n",
    "if train_available:\n",
    "    metric_scores = evaluate_conditional_ensemble(models, t1, t2, test_loader, device, qrels_test, queries_test)\n",
    "else:\n",
    "    metric_scores = evaluate_conditional_ensemble(models, t1, t2, test_loader, device, split_qrels_test, split_queries_test)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f'Metric {metric} score: {metric_scores[metric]:.4f}')\n",
    "\n",
    "results['ens-select'] = metric_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T15:09:47.983221900Z",
     "start_time": "2025-04-01T15:05:56.942385900Z"
    }
   },
   "id": "ea3b718cf8c07a1e"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully written results to results/huawei-noah/TinyBERT_General_4L_312D/fiqa\\ensemble-selective.txt.\n"
     ]
    }
   ],
   "source": [
    "save_dir = f\"results/{model_name}/{dataset_name}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'ensemble-selective.txt')\n",
    "\n",
    "write_results(metric_scores, save_path, model_name, dataset_name, length_setting)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-01T15:09:47.990889900Z",
     "start_time": "2025-04-01T15:09:47.984727100Z"
    }
   },
   "id": "97049e4b54bda9e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate on ensemble of models (Weighted average method)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cf845baf1869443"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_dir = f'models\\\\{model_name}\\\\{dataset_name}'\n",
    "models = load_models(model_dir, model_name, device)\n",
    "\n",
    "weights_config = {'short': [0.4, 0.3, 0.3], # Weights for [short, medium, long] models when query is short\n",
    "                  'medium': [0.3, 0.4, 0.3],# Weights when query is medium\n",
    "                  'long': [0.3, 0.3, 0.4]  # Weights when query is long\n",
    "                 }\n",
    "\n",
    "if train_available:\n",
    "    metric_scores = evaluate_weighted_average_ensemble(models, weights_config, t1, t2, test_loader, device, qrels_test, queries_test)\n",
    "else:\n",
    "    metric_scores = evaluate_weighted_average_ensemble(models, weights_config, t1, t2, test_loader, device, split_qrels_test, split_queries_test)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f'Metric {metric} score: {metric_scores[metric]:.4f}')\n",
    "\n",
    "results['ens-weighted'] = metric_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-01T15:09:47.992891Z"
    }
   },
   "id": "21d25cc97dc75416"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_dir = f\"results/{model_name}/{dataset_name}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'ensemble-weighted.txt')\n",
    "\n",
    "write_results(metric_scores, save_path, model_name, dataset_name, length_setting)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d0b3fc98aea012e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8c4e33e932103f31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate on ensemble of models (Weighted average method + regression on weights)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1463aa1d84aab17e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Precompute scores on the validation set\n",
    "print(\"\\n--- Optimizing Ensemble Weights using Validation Set ---\")\n",
    "models_all = load_models(model_dir, model_name, device) # Reload or ensure models are available\n",
    "precomputed_val_scores = precompute_validation_scores(models_all, val_loader, device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74893428f4eb500a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the single optimal weights configuration using the validation set\n",
    "weight_opt_trials = 300 # Or more, e.g., 100-300 depending on time\n",
    "\n",
    "learned_weights_config = find_optimal_weights_config(\n",
    "    precomputed_val_scores,\n",
    "    query_val,\n",
    "    qrels_val,\n",
    "    t1, t2,\n",
    "    metric_to_optimize=metric_to_optimize_weights, # NDCG@10\n",
    "    n_trials=weight_opt_trials,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "print(\"\\nLearned Weights Config:\")\n",
    "print(learned_weights_config)\n",
    "\n",
    "# Evaluate on the TEST set using the single learned weights config\n",
    "print(\"\\n--- Evaluating on TEST set using LEARNED weights configuration ---\")\n",
    "# Models should still be loaded in models_all\n",
    "if train_available:\n",
    "    metric_scores_learned_w = evaluate_weighted_average_ensemble(models_all, learned_weights_config, t1, t2, test_loader, device, qrels_test, queries_test)\n",
    "else:\n",
    "    metric_scores_learned_w = evaluate_weighted_average_ensemble(models_all, learned_weights_config, t1, t2, test_loader, device, split_qrels_test, split_queries_test)\n",
    "\n",
    "print(\"\\nFinal Test Set Performance with Learned Weights:\")\n",
    "for metric in metrics:\n",
    "     print(f'Metric {metric} score: {metric_scores_learned_w[metric]:.4f}')\n",
    "\n",
    "results['ens-learned-weighted'] = metric_scores_learned_w\n",
    "\n",
    "# Save learned weighted results\n",
    "save_dir = f\"results/{model_name}/{dataset_name}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'ensemble-weighted-reg.txt')\n",
    "write_results(metric_scores_learned_w, save_path, model_name, dataset_name, \"learned-weighted-config\") # Updated description"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb47af0132f3dc51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69ea521fdb609816"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2ddc9417f5e15f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_comparison_plot(results, metrics, model_name, dataset_name, save_dir)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f968f501c108fe30"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
