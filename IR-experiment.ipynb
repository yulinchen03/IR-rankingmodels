{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-22T16:17:07.531692300Z",
     "start_time": "2025-03-22T16:16:59.704512900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chena\\PycharmProjects\\IR-rankingmodels\\.venv\\Lib\\site-packages\\beir\\util.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/25657 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f0b0fe63d494e4184cdafa765fb50ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "from torch.optim import AdamW\n",
    "from ir_measures import nDCG, AP, P, R, RR\n",
    "from IRutils import dataprocessor, models, train, inference\n",
    "from IRutils.dataset import TripletRankingDataset\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# Download and unzip the dataset\n",
    "dataset_name = \"scidocs\"\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Create dataset for a specific query length range (e.g., short queries)\n",
    "length_setting = 'medium'\n",
    "ranges = {'short': (1, 6), 'medium': (7, 10), 'long': (11, sys.maxsize) }\n",
    "\n",
    "metrics = [nDCG@10, nDCG@100, AP@10, AP@100, P@10, R@10, P@100, R@100, RR]\n",
    "\n",
    "max_len_doc = 512  # max token length\n",
    "\n",
    "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset_name}.zip\"\n",
    "data_path = util.download_and_unzip(url, \"datasets\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the dataset\n",
    "docs, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T16:17:07.828061700Z",
     "start_time": "2025-03-22T16:17:07.531692300Z"
    }
   },
   "id": "d5b3d6619b369b9a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example query from medium subset:\n",
      "('89e58773fa59ef5b57f229832c2a1b3e3efff37e', 'Analyzing EEG signals to detect unexpected obstacles during walking')\n"
     ]
    }
   ],
   "source": [
    "range = ranges[length_setting]\n",
    "\n",
    "dp = dataprocessor.DataProcessor(queries, docs, qrels)\n",
    "query_subset, qrels_subset = dp.get_subset(range[0], range[1])  # Adjust min/max length\n",
    "\n",
    "print(f'Example query from {length_setting} subset:\\n{query_subset.popitem()}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T16:17:07.834562200Z",
     "start_time": "2025-03-22T16:17:07.829064800Z"
    }
   },
   "id": "61f7a4264e1d88ef"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of dataset: 1000\n",
      "Length of subset of medium queries: 481\n",
      "Length of subset of medium training queries: 269\n"
     ]
    }
   ],
   "source": [
    "query_train, query_val, query_test, qrel_train, qrel_val, qrel_test = dp.train_val_test_split(train_ratio=0.8, val_ratio=0.2, test_ratio = 0.3, random_state=42)  # adjust if needed\n",
    "\n",
    "print(f'Total length of dataset: {len(queries)}')\n",
    "print(f'Length of subset of {length_setting} queries: {len(query_subset)}')\n",
    "print(f'Length of subset of {length_setting} training queries: {len(query_train)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T16:17:07.875507Z",
     "start_time": "2025-03-22T16:17:07.833557300Z"
    }
   },
   "id": "2d4befbe86e3ba9a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "qrel_scores = list(qrels.values())\n",
    "relevance_scores = [list(item.values()) for item in qrel_scores]\n",
    "num_negatives = relevance_scores[0].count(0)\n",
    "print(num_negatives)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T16:17:07.875507Z",
     "start_time": "2025-03-22T16:17:07.850101200Z"
    }
   },
   "id": "f994281e36a5b927"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_dataset = TripletRankingDataset(query_train, docs, qrel_train, tokenizer, num_negatives, max_length=max_len_doc)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "val_dataset = TripletRankingDataset(query_val, docs, qrel_val, tokenizer, num_negatives,max_length=max_len_doc)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = TripletRankingDataset(query_test, docs, qrel_test, tokenizer, num_negatives,max_length=max_len_doc)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T16:17:08.000095300Z",
     "start_time": "2025-03-22T16:17:07.856502300Z"
    }
   },
   "id": "66d2c4809c9f3d5d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.TripletRankerModel(model_name).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "model_path = os.path.join(os.getcwd(), f'models/{model_name}+{dataset_name}+{length_setting}_queries.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T16:17:08.429144200Z",
     "start_time": "2025-03-22T16:17:08.000095300Z"
    }
   },
   "id": "90d4dda153d8d4a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chena\\PycharmProjects\\IR-rankingmodels\\models/distilbert-base-uncased+scidocs+medium_queries.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 1/10 (Training):   0%|          | 0/4157 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4560784f7431416983114062e61c5898"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model_path)\n",
    "if os.path.isfile(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "else:\n",
    "    # Train the model\n",
    "    model = train.train_triplet_ranker(model, train_loader, val_loader, optimizer, device, model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-03-22T16:17:08.429144200Z"
    }
   },
   "id": "61696e2993e57d70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example usage (replace with your data and model)\n",
    "metric_scores = inference.evaluate(model, test_loader, device, qrel_test)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f'Metric {metric} score: {metric_scores[metric]:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "156e9f0f994a8821"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save results to a file\n",
    "with open(f\"results/{model_name}+{dataset_name}+{length_setting}_queries.txt\", \"w\") as f:\n",
    "    f.write(f\"Evaluation Results for {model_name} model finetuned on {length_setting} queries from {dataset_name} dataset:\\n\")\n",
    "    f.write(f\"normalized Discounted Cumulative Gain@10: {metric_scores[nDCG@10]:.4f}\\n\")\n",
    "    f.write(f\"normalized Discounted Cumulative Gain@100: {metric_scores[nDCG@100]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"[Mean] Average Precision@10: {metric_scores[AP@10]:.4f}\\n\")\n",
    "    f.write(f\"[Mean] Average Precision@100: {metric_scores[AP@100]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Precision@10: {metric_scores[P@10]:.4f}\\n\")\n",
    "    f.write(f\"Recall@10: {metric_scores[R@10]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Precision@100: {metric_scores[P@100]:.4f}\\n\")\n",
    "    f.write(f\"Recall@100: {metric_scores[R@100]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"[Mean] Reciprocal Rank: {metric_scores[RR]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"----------------------------------------------------\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Explanation of metrics:\\n\")\n",
    "    f.write(f\"NDCG@k (Normalized Discounted Cumulative Gain: Ranking Quality | Prioritizes highly relevant documents appearing earlier in the ranking.\\n\")\n",
    "    f.write(f\"MAP (Mean Average Precision): Overall Relevance | Measures ranking precision across all relevant documents. Best for small-scale retrieval tasks.\\n\")\n",
    "    f.write(f\"Precision@k: Relevance | Measures how many of the top-k documents are relevant. Works well in precision-sensitive applications.\\n\")\n",
    "    f.write(f\"Recall@k: Coverage | Measures how many relevant documents appear in the top-k results. Important in recall-sensitive tasks.\\n\")\n",
    "    f.write(f\"MRR (Mean Reciprocal Rank): Single Relevant Result | Focuses on ranking the first relevant document. Good for QA tasks.\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "44e782fd22fb4942"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
