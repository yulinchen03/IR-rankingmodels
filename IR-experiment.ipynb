{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Getting started (Adjust settings to your experiment's needs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "317b36640a1a5743"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-22T18:34:13.386949100Z",
     "start_time": "2025-03-22T18:34:03.999724600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chena\\PycharmProjects\\IR-rankingmodels\\.venv\\Lib\\site-packages\\beir\\util.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5183 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82c3b41616a044ac966ae060ed0f18a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5183 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29cacfa19cde4ddf881cfad7ee19278e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from ir_measures import nDCG, AP, P, R, RR\n",
    "from IRutils import dataprocessor, models, train, inference\n",
    "from IRutils.dataset import TripletRankingDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#################### THINGS TO CHANGE FOR YOUR EXPERIMENTS ####################\n",
    "\n",
    "dataset_name = \"scifact\"  # SELECT YOUR EXPERIMENT DATASET HERE\n",
    "model_name = \"distilbert-base-uncased\"  # SELECT YOUR MODEL HERE\n",
    "\n",
    "# Create dataset for a specific query length range (e.g., short queries)\n",
    "length_setting = 'medium'\n",
    "ranges = {'short': (1, 6), 'medium': (7, 10), 'long': (11, sys.maxsize) }\n",
    "\n",
    "metrics = [nDCG@10, nDCG@100, AP@10, AP@100, P@10, R@10, P@100, R@100, RR]\n",
    "\n",
    "#################### THINGS TO CHANGE FOR YOUR EXPERIMENTS ####################\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "datasets = {'msmarco': ['train', 'dev'],\n",
    "            'hotpotqa': ['train', 'dev', 'test'],\n",
    "            'arguana': ['test'],\n",
    "            'quora': ['dev', 'test'],\n",
    "            'scidocs': ['test'],  # small\n",
    "            'fever': ['train', 'dev', 'test'],  # large\n",
    "            'climate-fever': ['test'],\n",
    "            'scifact': ['train', 'test'],\n",
    "            'fiqa': ['train', 'dev', 'test'],\n",
    "            'nfcorpus': ['train', 'dev', 'test']\n",
    "            }\n",
    "\n",
    "max_len_doc = 512  # max token length\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download and unzip the dataset\n",
    "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset_name}.zip\"\n",
    "data_path = util.download_and_unzip(url, \"datasets\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_available = False\n",
    "if 'train' in datasets[dataset_name]:\n",
    "    # Load the dataset\n",
    "    docs, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"train\")\n",
    "    docs_test, queries_test, qrels_test = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "    train_available = True\n",
    "else:\n",
    "    # Load the dataset\n",
    "    docs, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3e7ba21e8cd266b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T18:34:13.582801900Z",
     "start_time": "2025-03-22T18:34:13.384838700Z"
    }
   },
   "id": "d5b3d6619b369b9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get test split (if no seperate test set available)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a7d9b8e67d5d99"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 809\n",
      "test size: 300\n"
     ]
    }
   ],
   "source": [
    "range = ranges[length_setting]\n",
    "\n",
    "dp = dataprocessor.DataProcessor(queries, docs, qrels)\n",
    "\n",
    "print(f'Dataset size: {len(queries)}')\n",
    "\n",
    "# first seperate the test set (include queries of all lengths)\n",
    "if not train_available:\n",
    "    query_test, qrel_test = dp.get_testset(test_ratio=0.2, random_state=random_state)\n",
    "    print(f'test size: {len(query_test)}')\n",
    "else:\n",
    "    print(f'test size: {len(queries_test)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T18:34:13.589400300Z",
     "start_time": "2025-03-22T18:34:13.583800900Z"
    }
   },
   "id": "61f7a4264e1d88ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query length filtering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a14477b476a15142"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# filter by query length\n",
    "query_subset, qrels_subset = dp.get_subset(range[0], range[1])  # Adjust min/max length"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T18:34:13.596069Z",
     "start_time": "2025-03-22T18:34:13.588403200Z"
    }
   },
   "id": "6a3321bda81a637b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Val split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "442e5dbf79a69d79"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example query from medium subset:\n",
      "('1406', 'β-sheet opening occurs during pleurotolysin pore formation.')\n",
      "Length of subset of medium validation queries: 53\n",
      "Length of subset of medium training queries: 210\n",
      "Length of subset of medium queries: 263\n"
     ]
    }
   ],
   "source": [
    "print(f'Example query from {length_setting} subset:\\n{query_subset.popitem()}')\n",
    "\n",
    "query_train, query_val, qrel_train, qrel_val = dp.train_val_split(train_ratio=0.8, val_ratio=0.2, train_available=train_available, random_state=42)  # adjust if needed\n",
    "\n",
    "print(f'Length of subset of medium validation queries: {len(query_val)}')\n",
    "print(f'Length of subset of {length_setting} training queries: {len(query_train)}')\n",
    "print(f'Length of subset of {length_setting} queries: {len(query_subset)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T18:34:13.602355900Z",
     "start_time": "2025-03-22T18:34:13.596069Z"
    }
   },
   "id": "2d4befbe86e3ba9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check if the qrels already contain negative samples （If not create later)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4770b5618b545f18"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "qrel_scores = list(qrels.values()) \n",
    "relevance_scores = [list(item.values()) for item in qrel_scores]\n",
    "num_negatives = relevance_scores[0].count(0)\n",
    "print(num_negatives)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T18:34:13.635982900Z",
     "start_time": "2025-03-22T18:34:13.603357Z"
    }
   },
   "id": "f994281e36a5b927"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create datasets and data loaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58324ddeb0b3c7bf"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:00<00:00, 1381.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 5058.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating testing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 5213.96it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Creating training dataset...')\n",
    "train_dataset = TripletRankingDataset(query_train, docs, qrel_train, tokenizer, num_negatives, max_length=max_len_doc)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "print('Creating validation dataset...')\n",
    "val_dataset = TripletRankingDataset(query_val, docs, qrel_val, tokenizer, num_negatives,max_length=max_len_doc)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "print('Creating testing dataset...')\n",
    "if train_available:\n",
    "    test_dataset = TripletRankingDataset(queries_test, docs_test, qrels_test, tokenizer, num_negatives,max_length=max_len_doc)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "else:\n",
    "    test_dataset = TripletRankingDataset(query_test, docs, qrel_test, tokenizer, num_negatives,max_length=max_len_doc) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T18:34:13.847223700Z",
     "start_time": "2025-03-22T18:34:13.610899800Z"
    }
   },
   "id": "66d2c4809c9f3d5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33f7b31408fdfbf7"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.TripletRankerModel(model_name).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "model_path = os.path.join(os.getcwd(), f'models/{model_name}+{dataset_name}+{length_setting}_queries.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T18:34:14.330970100Z",
     "start_time": "2025-03-22T18:34:13.843679500Z"
    }
   },
   "id": "90d4dda153d8d4a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train model (load directly if already trained)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b346b91371f8ee56"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chena\\PycharmProjects\\IR-rankingmodels\\models/distilbert-base-uncased+scifact+medium_queries.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 1/10 (Training):   0%|          | 0/595 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27c5aea724104e90af6e1064025dcf33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Training Loss: 0.6046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 153/153 [00:16<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5743\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 2/10 (Training):   0%|          | 0/595 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48206e67cc3a421ba7a72c3e10959c98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Average Training Loss: 0.5425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 153/153 [00:16<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5457\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 3/10 (Training):   0%|          | 0/595 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecdbfb1ceb9046cdb250c87d06bbe695"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Average Training Loss: 0.4183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 153/153 [00:16<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3761\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 4/10 (Training):   0%|          | 0/595 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe1a12ca5e3d43a9851e6428d995f6b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Average Training Loss: 0.1768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 153/153 [00:16<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3640\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 5/10 (Training):   0%|          | 0/595 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "597551485af34412be253780644d7fff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Average Training Loss: 0.1320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 153/153 [00:16<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4467\n",
      "Validation loss did not improve. Patience: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 6/10 (Training):   0%|          | 0/595 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab88084682a04fcf8923efd52fbd057f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Average Training Loss: 0.0807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 153/153 [00:16<00:00,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3974\n",
      "Validation loss did not improve. Patience: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 7/10 (Training):   0%|          | 0/595 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57f690d1e528455687b86973e90b4a4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Average Training Loss: 0.0993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 153/153 [00:16<00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2897\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 8/10 (Training):   0%|          | 0/595 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4602b5772fc5476dba217877f62a7671"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Average Training Loss: 0.0526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 153/153 [00:16<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2201\n",
      "Validation loss improved. Saving model.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 9/10 (Training):   0%|          | 0/595 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00241e74c3f842169ad32c8058379844"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Average Training Loss: 0.0736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 153/153 [00:16<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3975\n",
      "Validation loss did not improve. Patience: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 10/10 (Training):   0%|          | 0/595 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60dcb0bed51c41dda5bb324208a7e937"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Average Training Loss: 0.0641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 153/153 [00:16<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2573\n",
      "Validation loss did not improve. Patience: 2/3\n",
      "Loaded best model based on validation loss.\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "print(model_path)\n",
    "if os.path.isfile(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "else:\n",
    "    # Train the model\n",
    "    model = train.train_triplet_ranker(model, train_loader, val_loader, optimizer, device, model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T19:07:20.604179300Z",
     "start_time": "2025-03-22T18:34:14.328408100Z"
    }
   },
   "id": "61696e2993e57d70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run inference on test set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "374c2aab9651bcba"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 848/848 [01:32<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric nDCG@10 score: 0.6881\n",
      "Metric nDCG@100 score: 0.7235\n",
      "Metric AP@10 score: 0.6300\n",
      "Metric AP@100 score: 0.6392\n",
      "Metric P@10 score: 0.0947\n",
      "Metric R@10 score: 0.8586\n",
      "Metric P@100 score: 0.0113\n",
      "Metric R@100 score: 1.0000\n",
      "Metric RR score: 0.6447\n"
     ]
    }
   ],
   "source": [
    "# Example usage (replace with your data and model)\n",
    "if train_available:\n",
    "    metric_scores = inference.evaluate(model, test_loader, device, qrels_test)\n",
    "else:\n",
    "    metric_scores = inference.evaluate(model, test_loader, device, qrel_test)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f'Metric {metric} score: {metric_scores[metric]:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T19:08:52.800357100Z",
     "start_time": "2025-03-22T19:07:20.597180200Z"
    }
   },
   "id": "156e9f0f994a8821"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write results to output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e93bcfd6f67ceca"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Save results to a file\n",
    "with open(f\"results/{model_name}+{dataset_name}+{length_setting}_queries.txt\", \"w\") as f:\n",
    "    f.write(f\"Evaluation Results for {model_name} model finetuned on {length_setting} queries from {dataset_name} dataset:\\n\")\n",
    "    f.write(f\"normalized Discounted Cumulative Gain@10: {metric_scores[nDCG@10]:.4f}\\n\")\n",
    "    f.write(f\"normalized Discounted Cumulative Gain@100: {metric_scores[nDCG@100]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"[Mean] Average Precision@10: {metric_scores[AP@10]:.4f}\\n\")\n",
    "    f.write(f\"[Mean] Average Precision@100: {metric_scores[AP@100]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Precision@10: {metric_scores[P@10]:.4f}\\n\")\n",
    "    f.write(f\"Recall@10: {metric_scores[R@10]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Precision@100: {metric_scores[P@100]:.4f}\\n\")\n",
    "    f.write(f\"Recall@100: {metric_scores[R@100]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"[Mean] Reciprocal Rank: {metric_scores[RR]:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"----------------------------------------------------\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Explanation of metrics:\\n\")\n",
    "    f.write(f\"NDCG@k (Normalized Discounted Cumulative Gain: Ranking Quality | Prioritizes highly relevant documents appearing earlier in the ranking.\\n\")\n",
    "    f.write(f\"MAP (Mean Average Precision): Overall Relevance | Measures ranking precision across all relevant documents. Best for small-scale retrieval tasks.\\n\")\n",
    "    f.write(f\"Precision@k: Relevance | Measures how many of the top-k documents are relevant. Works well in precision-sensitive applications.\\n\")\n",
    "    f.write(f\"Recall@k: Coverage | Measures how many relevant documents appear in the top-k results. Important in recall-sensitive tasks.\\n\")\n",
    "    f.write(f\"MRR (Mean Reciprocal Rank): Single Relevant Result | Focuses on ranking the first relevant document. Good for QA tasks.\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T19:08:52.809897500Z",
     "start_time": "2025-03-22T19:08:52.800357100Z"
    }
   },
   "id": "44e782fd22fb4942"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
