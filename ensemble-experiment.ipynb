{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-26T23:03:17.227470900Z",
     "start_time": "2025-03-26T23:03:10.106038300Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "from IRutils.models import TripletRankerModel\n",
    "from ir_measures import calc_aggregate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def load_model(path, device):\n",
    "    model = TripletRankerModel(model_name).to(device=device)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def evaluate(models, test_loader, device, qrels):\n",
    "    run = {}  # Format: {qid: {doc_id: score}}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            qids = batch[\"qid\"]\n",
    "            pos_dids = batch[\"pos_did\"]\n",
    "            neg_dids = batch[\"neg_did\"]\n",
    "\n",
    "            # Process embeddings and calculate distances\n",
    "            anchor_inputs = batch[\"anchor_input_ids\"].to(device)\n",
    "            anchor_masks = batch[\"anchor_attention_mask\"].to(device)\n",
    "            positive_inputs = batch[\"positive_input_ids\"].to(device)\n",
    "            positive_masks = batch[\"positive_attention_mask\"].to(device)\n",
    "            negative_inputs = batch[\"negative_input_ids\"].to(device)\n",
    "            negative_masks = batch[\"negative_attention_mask\"].to(device)\n",
    "            \n",
    "            pd = []\n",
    "            nd = []\n",
    "            \n",
    "            for model in models:\n",
    "                model.eval()\n",
    "                model.to(device)\n",
    "                \n",
    "                anchor_embeddings = model(anchor_inputs, anchor_masks)\n",
    "                positive_embeddings = model(positive_inputs, positive_masks)\n",
    "                negative_embeddings = model(negative_inputs, negative_masks)\n",
    "    \n",
    "                pos_distances = torch.norm(anchor_embeddings - positive_embeddings, p=2, dim=1)\n",
    "                neg_distances = torch.norm(anchor_embeddings - negative_embeddings, p=2, dim=1)\n",
    "                \n",
    "                pd.append(pos_distances)\n",
    "                nd.append(neg_distances)\n",
    "                \n",
    "            # Compute the average distances across all three models\n",
    "            final_pos_distances = torch.stack(pd).mean(dim=0)  # Average over models\n",
    "            final_neg_distances = torch.stack(nd).mean(dim=0)  # Average over models\n",
    "\n",
    "            # Build the run dictionary\n",
    "            i = 0\n",
    "            while i < len(qids):\n",
    "                qid = qids[i]\n",
    "                pos_did = pos_dids[i]\n",
    "                neg_did = neg_dids[i]\n",
    "\n",
    "                pos_score = -final_pos_distances[i].item()\n",
    "                neg_score = -final_neg_distances[i].item()\n",
    "\n",
    "                if qid not in run:\n",
    "                    run[qid] = {}\n",
    "\n",
    "                # Add scores directly (no list of dicts)\n",
    "                run[qid][pos_did] = pos_score\n",
    "                run[qid][neg_did] = neg_score\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = [\n",
    "        nDCG @ 10, nDCG @ 100,\n",
    "        AP @ 10, AP @ 100,\n",
    "        P @ 10, R @ 10,\n",
    "        P @ 100, R @ 100,\n",
    "        RR\n",
    "    ]\n",
    "\n",
    "    metric_scores = calc_aggregate(metrics, qrels, run)\n",
    "\n",
    "    return metric_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T23:03:17.246052700Z",
     "start_time": "2025-03-26T23:03:17.233473100Z"
    }
   },
   "id": "506b7d495c7d86b8"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "dataset_name = 'fiqa'\n",
    "\n",
    "model_dir = f'models\\\\{model_name}\\\\{dataset_name}'\n",
    "model_paths = [os.path.join(model_dir, name) for name in os.listdir(model_dir) if \"full\" not in name]\n",
    "models = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for path in model_paths:\n",
    "    model = load_model(path, device)\n",
    "    models.append(model)\n",
    "    \n",
    "print(len(models))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T23:03:19.802578200Z",
     "start_time": "2025-03-26T23:03:17.241053100Z"
    }
   },
   "id": "af699fb18d36e1d4"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/57638 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43e2307a124348f08d8690871e627dad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/57638 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3cfb5cfb46b4ffa94ec57055c20bdcb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test set available!\n"
     ]
    }
   ],
   "source": [
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir import util\n",
    "\n",
    "datasets = {'msmarco': ['train', 'dev'],\n",
    "            'hotpotqa': ['train', 'dev', 'test'],\n",
    "            'arguana': ['test'],\n",
    "            'quora': ['dev', 'test'],\n",
    "            'scidocs': ['test'],  # small\n",
    "            'fever': ['train', 'dev', 'test'],  # large\n",
    "            'climate-fever': ['test'],\n",
    "            'scifact': ['train', 'test'],\n",
    "            'fiqa': ['train', 'dev', 'test'],\n",
    "            'nfcorpus': ['train', 'dev', 'test']\n",
    "            }\n",
    "\n",
    "# Download and unzip the dataset\n",
    "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset_name}.zip\"\n",
    "data_path = util.download_and_unzip(url, \"datasets\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_available = False\n",
    "if 'train' in datasets[dataset_name]:\n",
    "    # Load the dataset\n",
    "    docs, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"train\")\n",
    "    docs_test, queries_test, qrels_test = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "    train_available = True\n",
    "    print('Train and test set available!')\n",
    "else:\n",
    "    # Load the dataset\n",
    "    docs, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "    print('Only test set available!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T23:03:20.577942800Z",
     "start_time": "2025-03-26T23:03:19.794571200Z"
    }
   },
   "id": "8ac5915e41124456"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T23:03:20.731073900Z",
     "start_time": "2025-03-26T23:03:20.570943200Z"
    }
   },
   "id": "7bd9247d9a5cddd"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_percentiles(query_lengths):\n",
    "    # Calculate the percentiles\n",
    "    t1 = np.percentile(query_lengths, 33)\n",
    "    t2 = np.percentile(query_lengths, 67)\n",
    "    return int(t1), int(t2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T23:03:20.739305400Z",
     "start_time": "2025-03-26T23:03:20.732075700Z"
    }
   },
   "id": "c6582913a492b4ad"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5500\n",
      "test size: 648\n"
     ]
    }
   ],
   "source": [
    "from IRutils import dataprocessor\n",
    "\n",
    "dp = dataprocessor.DataProcessor(queries, docs, qrels)\n",
    "\n",
    "print(f'Dataset size: {len(queries)}')\n",
    "\n",
    "# first seperate the test set (include queries of all lengths)\n",
    "if not train_available:\n",
    "    query_test, qrel_test = dp.get_testset(test_ratio=0.2, random_state=42)\n",
    "    print(f'test size: {len(query_test)}')\n",
    "else:\n",
    "    print(f'test size: {len(queries_test)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T23:03:20.747011400Z",
     "start_time": "2025-03-26T23:03:20.736308400Z"
    }
   },
   "id": "32ef456ce8b57fb9"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negatives in qrels: 0\n",
      "Creating testing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 648/648 [00:01<00:00, 328.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from IRutils.dataset import TripletRankingDataset\n",
    "\n",
    "qrel_scores = list(qrels.values()) \n",
    "relevance_scores = [list(item.values()) for item in qrel_scores]\n",
    "num_negatives = relevance_scores[0].count(0)\n",
    "print(f'Number of negatives in qrels: {num_negatives}')\n",
    "\n",
    "print('Creating testing dataset...')\n",
    "if train_available:\n",
    "    test_dataset = TripletRankingDataset(queries_test, docs_test, qrels_test, tokenizer, num_negatives,max_length=512)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "else:\n",
    "    test_dataset = TripletRankingDataset(query_test, docs, qrel_test, tokenizer, num_negatives,max_length=512) \n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T23:03:22.733041400Z",
     "start_time": "2025-03-26T23:03:20.747011400Z"
    }
   },
   "id": "a169b1ee198278d4"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluating:   0%|          | 0/4265 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fe7f38e52564c968fa8cd0765d1cbf3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric nDCG@10 score: 0.7402\n",
      "Metric nDCG@100 score: 0.7802\n",
      "Metric AP@10 score: 0.6417\n",
      "Metric AP@100 score: 0.6644\n",
      "Metric P@10 score: 0.2106\n",
      "Metric R@10 score: 0.8857\n",
      "Metric P@100 score: 0.0261\n",
      "Metric R@100 score: 0.9977\n",
      "Metric RR score: 0.7430\n"
     ]
    }
   ],
   "source": [
    "from IRutils import inference\n",
    "from ir_measures import nDCG, AP, P, R, RR\n",
    "\n",
    "metrics = [nDCG@10, nDCG@100, AP@10, AP@100, P@10, R@10, P@100, R@100, RR]\n",
    "\n",
    "# Example usage (replace with your data and model)\n",
    "if train_available:\n",
    "    metric_scores = evaluate(models, test_loader, device, qrels_test)\n",
    "else:\n",
    "    metric_scores = evaluate(models, test_loader, device, qrel_test)\n",
    "    \n",
    "for metric in metrics:\n",
    "    print(f'Metric {metric} score: {metric_scores[metric]:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T23:54:19.984006800Z",
     "start_time": "2025-03-26T23:03:22.728838600Z"
    }
   },
   "id": "b23ee5c92ee4ae94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble test 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfcf743f334c5d77"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def evaluate(models, test_loader, device, qrels):\n",
    "    run = {}  # Format: {qid: {doc_id: score}}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            qids = batch[\"qid\"]\n",
    "            pos_dids = batch[\"pos_did\"]\n",
    "            neg_dids = batch[\"neg_did\"]\n",
    "\n",
    "            # Process embeddings and calculate distances\n",
    "            anchor_inputs = batch[\"anchor_input_ids\"].to(device)\n",
    "            anchor_masks = batch[\"anchor_attention_mask\"].to(device)\n",
    "            positive_inputs = batch[\"positive_input_ids\"].to(device)\n",
    "            positive_masks = batch[\"positive_attention_mask\"].to(device)\n",
    "            negative_inputs = batch[\"negative_input_ids\"].to(device)\n",
    "            negative_masks = batch[\"negative_attention_mask\"].to(device)\n",
    "            \n",
    "            pd = []\n",
    "            nd = []\n",
    "            \n",
    "            for model in models:\n",
    "                model.eval()\n",
    "                model.to(device)\n",
    "                \n",
    "                anchor_embeddings = model(anchor_inputs, anchor_masks)\n",
    "                positive_embeddings = model(positive_inputs, positive_masks)\n",
    "                negative_embeddings = model(negative_inputs, negative_masks)\n",
    "    \n",
    "                pos_distances = torch.norm(anchor_embeddings - positive_embeddings, p=2, dim=1)\n",
    "                neg_distances = torch.norm(anchor_embeddings - negative_embeddings, p=2, dim=1)\n",
    "                \n",
    "                pd.append(pos_distances)\n",
    "                nd.append(neg_distances)\n",
    "                \n",
    "            # Compute the average distances across all three models\n",
    "            final_pos_distances = torch.stack(pd).mean(dim=0)  # Average over models\n",
    "            final_neg_distances = torch.stack(nd).mean(dim=0)  # Average over models\n",
    "\n",
    "            # Build the run dictionary\n",
    "            i = 0\n",
    "            while i < len(qids):\n",
    "                qid = qids[i]\n",
    "                pos_did = pos_dids[i]\n",
    "                neg_did = neg_dids[i]\n",
    "\n",
    "                pos_score = -final_pos_distances[i].item()\n",
    "                neg_score = -final_neg_distances[i].item()\n",
    "\n",
    "                if qid not in run:\n",
    "                    run[qid] = {}\n",
    "\n",
    "                # Add scores directly (no list of dicts)\n",
    "                run[qid][pos_did] = pos_score\n",
    "                run[qid][neg_did] = neg_score\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = [\n",
    "        nDCG @ 10, nDCG @ 100,\n",
    "        AP @ 10, AP @ 100,\n",
    "        P @ 10, R @ 10,\n",
    "        P @ 100, R @ 100,\n",
    "        RR\n",
    "    ]\n",
    "\n",
    "    metric_scores = calc_aggregate(metrics, qrels, run)\n",
    "\n",
    "    return metric_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-26T23:54:19.988220400Z",
     "start_time": "2025-03-26T23:54:19.986007400Z"
    }
   },
   "id": "5e6a49f44ccb37fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
