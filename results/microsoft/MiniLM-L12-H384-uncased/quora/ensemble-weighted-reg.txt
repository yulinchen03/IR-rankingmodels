Evaluation Results for microsoft/MiniLM-L12-H384-uncased model (learned-weighted-config) on quora dataset:
----------------------------------------------------
nDCG@3:  0.9863
nDCG@5:  0.9866
nDCG@10: 0.9873
MRR:     0.9872 ([Mean] Reciprocal Rank)

P@1:     0.9835
P@3:     0.4422
P@5:     0.2875

R@1:     0.8395
R@3:     0.9566
R@5:     0.9754
R@10:    0.9879

----------------------------------------------------

Explanation of reported metrics:
  nDCG@k: Measures ranking quality, rewarding highly relevant documents found earlier.
          Normalized for the number of relevant items per query. Good overall indicator.
  MRR:    Average reciprocal rank of the *first* relevant document. Crucial for sparse relevance.
  P@k:    Precision@k. Fraction of top k results that are relevant. P@1 is very important.
  R@k:    Recall@k. Fraction of *all* relevant documents found in top k. Measures coverage.
          R@1, R@3, R@5 are useful for checking if the few relevant docs are found early.
