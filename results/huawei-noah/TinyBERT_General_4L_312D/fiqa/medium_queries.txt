Evaluation Results for huawei-noah/TinyBERT_General_4L_312D model finetuned on medium queries from fiqa dataset:
normalized Discounted Cumulative Gain@10: 0.6719
normalized Discounted Cumulative Gain@100: 0.7233

[Mean] Average Precision@10: 0.5642
[Mean] Average Precision@100: 0.5887

Precision@10: 0.1971
Recall@10: 0.8438

Precision@100: 0.0260
Recall@100: 0.9969

[Mean] Reciprocal Rank: 0.6648

----------------------------------------------------

Explanation of metrics:
NDCG@k (Normalized Discounted Cumulative Gain: Ranking Quality | Prioritizes highly relevant documents appearing earlier in the ranking.
MAP (Mean Average Precision): Overall Relevance | Measures ranking precision across all relevant documents. Best for small-scale retrieval tasks.
Precision@k: Relevance | Measures how many of the top-k documents are relevant. Works well in precision-sensitive applications.
Recall@k: Coverage | Measures how many relevant documents appear in the top-k results. Important in recall-sensitive tasks.
MRR (Mean Reciprocal Rank): Single Relevant Result | Focuses on ranking the first relevant document. Good for QA tasks.
